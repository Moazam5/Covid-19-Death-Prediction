import pandas as pdimport osimport numpy as np# Common importsimport numpy as npimport osfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedShuffleSplit# to make this notebook's output stable across runsnp.random.seed(42)# To plot pretty figures#%matplotlib inlineimport matplotlib as mplimport matplotlib.pyplot as pltmpl.rc('axes', labelsize=14)mpl.rc('xtick', labelsize=12)mpl.rc('ytick', labelsize=12)# Where to save the figuresPROJECT_ROOT_DIR = "/Users/moazam/Documents"CHAPTER_ID = "BigDataFinalProject"IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR,  CHAPTER_ID)def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)    print("Saving figure", fig_id)    if tight_layout:        plt.tight_layout()    plt.savefig(path, format=fig_extension, dpi=resolution)# Ignore useless warnings (see SciPy issue #5998)import warningswarnings.filterwarnings(action="ignore", message="^internal gelsd")HOUSING_PATH ='/Users/moazam/Downloads/'def load_housing_data(housing_path=HOUSING_PATH):    csv_path = os.path.join(housing_path, "us-counties.csv")    return pd.read_csv(csv_path)housing = load_housing_data()#%matplotlib inlineimport matplotlib.pyplot as plthousing.hist(bins=50, figsize=(20,15))save_fig("attribute_histogram_plots")plt.show()np.random.seed(42)#Create Test Set and Train Settrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)#Plothousing.plot(kind="scatter", x="cases", y="deaths", alpha=0.4,    s=housing["cases"]/100, label="Cases", figsize=(10,7),    c="deaths", cmap=plt.get_cmap("jet"), colorbar=True,    sharex=False)plt.legend()save_fig("housing_prices_scatterplot")#Look for co-relationscorr_matrix = housing.corr()corr_matrix["deaths"].sort_values(ascending=False)#Different Attribute Combinationshousing["deaths_fips"] = housing["deaths"]/housing["fips"]housing["cases_fips"] = housing["cases"]/housing["fips"]corr_matrix = housing.corr()corr_matrix["deaths"].sort_values(ascending=False)#Prepare for MLhousing_labels = housing["deaths"].copy()housing = housing.drop("deaths", axis=1) # drop labels for training settry:    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+except ImportError:    from sklearn.preprocessing import Imputer as SimpleImputerimputer = SimpleImputer(strategy="median")housing_num = housing.drop(['date','county','state'], axis=1)convert_dict = {'cases': float  }housing_num = housing_num.astype(convert_dict)sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()imputer.fit(housing_num)imputer.statistics_housing_num.median().valuesX = imputer.transform(housing_num)housing_tr = pd.DataFrame(X, columns=housing_num.columns,                          index=housing.index)housing_tr.loc[sample_incomplete_rows.index.values]housing_tr = pd.DataFrame(X, columns=housing_num.columns,                          index=housing_num.index)housing_tr.head()housing_cat = housing[['state','county']]housing_cat.head(10)try:    from sklearn.preprocessing import OrdinalEncoderexcept ImportError:    from future_encoders import OrdinalEncoder # Scikit-Learn < 0.20ordinal_encoder = OrdinalEncoder()housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)housing_cat_encoded[:10]ordinal_encoder.categories_try:    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20    from sklearn.preprocessing import OneHotEncoderexcept ImportError:    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20cat_encoder = OneHotEncoder()housing_cat_1hot = cat_encoder.fit_transform(housing_cat)housing_cat_1hothousing_cat_1hot.toarray()cat_encoder = OneHotEncoder(sparse=False)housing_cat_1hot = cat_encoder.fit_transform(housing_cat)housing_cat_1hotcat_encoder.categories_from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalernum_pipeline = Pipeline([        ('imputer', SimpleImputer(strategy="median")),        ('std_scaler', StandardScaler()),    ])housing_num_tr = num_pipeline.fit_transform(housing_num)try:    from sklearn.compose import ColumnTransformerexcept ImportError:    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20num_attribs = list(housing_num)cat_attribs = ["state","county"]full_pipeline = ColumnTransformer([        ("num", num_pipeline, num_attribs),        ("cat", OneHotEncoder(), cat_attribs),    ])housing_prepared = full_pipeline.fit_transform(housing)